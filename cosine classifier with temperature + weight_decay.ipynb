{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine-similarity classifier [PyTorch]\n",
    "- feature extractor: ConvNet\n",
    "- classifier: cosine classifier (S. Gidaris et al., 2018)\n",
    "\n",
    "<b>Goal</b>: See if it can learn the representations effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import relu\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU settings\n",
    "device = torch.device(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Learning Settings\n",
    "K-shot N-ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "N = 1\n",
    "left_class = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# import the `MNIST datasets`\n",
    "mnist_train = dsets.MNIST(root='data',\n",
    "                          train=True,\n",
    "                          transform=data_transform,\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='data',\n",
    "                          train=False,\n",
    "                          transform=data_transform,\n",
    "                          download=True)\n",
    "\n",
    "# build the `DataLoader`\n",
    "train_data_loader = DataLoader(mnist_train, batch_size=2**9)\n",
    "test_data_loader = DataLoader(mnist_test, batch_size=mnist_test.data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daesoo\\AppData\\Local\\Programs\\Python\\Python37\\venvs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Label Encoder\n",
    "label_encoder  = preprocessing.LabelEncoder()\n",
    "\n",
    "targets = list(range(0, 10, 1))\n",
    "targets.pop(left_class)\n",
    "targets = np.array(targets).reshape(-1, 1)\n",
    "\n",
    "label_encoder.fit(targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
      "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
      "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
      "        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
      "        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
      "        2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 5, 9, 1, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
      "        8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
      "        7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
      "        0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 7, 2, 7, 3,\n",
      "        0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
      "        8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3, 6, 4, 7, 5, 0, 6, 2, 7,\n",
      "        9, 8, 5, 9, 2, 1, 1, 4, 4, 5, 6, 4, 1, 2, 5, 3, 9, 3, 9, 0, 5, 9, 6, 5,\n",
      "        7, 4, 1, 3, 4, 0, 4, 8, 0, 4, 3, 6, 8, 7, 6, 0, 9, 7, 5, 7, 2, 1, 1, 6,\n",
      "        8, 9, 4, 1, 5, 2, 2, 9, 0, 3, 9, 6, 7, 2, 0, 3, 5, 4, 3, 6, 5, 8, 9, 5,\n",
      "        4, 7, 4, 2, 7, 3, 4, 8, 9, 1, 9, 2, 8, 7, 9, 1, 8, 7, 4, 1, 3, 1, 1, 0,\n",
      "        2, 3, 9, 4, 9, 2, 1, 6, 8, 4, 7, 7, 4, 4, 9, 2, 5, 7, 2, 4, 4, 2, 1, 9,\n",
      "        7, 2, 8, 7, 6, 9, 2, 2, 3, 8, 1, 6, 5, 1, 1, 0, 2, 6, 4, 5, 8, 3, 1, 5,\n",
      "        1, 9, 2, 7, 4, 4, 4, 8, 1, 5, 8, 9, 5, 6, 7, 9, 9, 3, 7, 0, 9, 0, 6, 6,\n",
      "        2, 3, 9, 0, 7, 5, 4, 8, 0, 9, 4, 1, 2, 8, 7, 1, 2, 6, 1, 0, 3, 0, 1, 1,\n",
      "        8, 2, 0, 3, 9, 4, 0, 5, 0, 6, 1, 7, 7, 8, 1, 9, 2, 0, 5, 1, 2, 2, 7, 3,\n",
      "        5, 4, 9, 7, 1, 8, 3, 9, 6, 0, 3, 1, 1, 2, 6, 3, 5, 7, 6, 8, 3, 9, 5, 8,\n",
      "        5, 7, 6, 1, 1, 3, 1, 7])\n",
      "tensor(1.)\n",
      "torch.Size([512, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_data_loader:\n",
    "    print(x); print(y); print(x.max()); print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size=28, embedding_feature_size=2, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Data properties\n",
    "        in_channels = 1\n",
    "        \n",
    "        # Define layers\n",
    "        # 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, 12, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.Dropout(0.2) #nn.BatchNorm2d(12)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 2\n",
    "        self.conv2 = nn.Conv2d(12, 24, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.Dropout(0.2) #nn.BatchNorm2d(24)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3\n",
    "        self.conv3 = nn.Conv2d(24, 48, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.Dropout(0.0) #nn.BatchNorm2d(48)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 4\n",
    "        self.last_embedding_layer = nn.Linear(48*3*3, embedding_feature_size)\n",
    "        \n",
    "        # Each `w` is a representation vector of each class.\n",
    "        self.Wstar_layer = nn.Linear(embedding_feature_size, n_classes, bias=False)  # 'bias=False' allows the weights to be the pure representation vectors (not affected by the bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #============================\n",
    "        # Feature Extractor\n",
    "        h1 = self.maxpool1(relu(self.bn1(self.conv1(x))))\n",
    "        h2 = self.maxpool2(relu(self.bn2(self.conv2(h1))))\n",
    "        h3 = self.maxpool3(relu(self.bn3(self.conv3(h2))))\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        h3 = h3.view(batch_size, -1)  # 1\n",
    "        \n",
    "        self.z = self.last_embedding_layer(h3)\n",
    "        norm_z = self.z / torch.norm(self.z, p=2, dim=1, keepdim=True)#.detach() # 2)\n",
    "        \n",
    "        # 1) flattne layer\n",
    "        # 2) representation vector of `x`; Note `.detach()`: ; We don't want the l2-norm function to be involved with the gradient descent update.\n",
    "        \n",
    "        #============================\n",
    "        # Cosine-similarity Classifier\n",
    "        Wstar = self.Wstar_layer.weight.T#.detach()  # Note `.detach()`\n",
    "        norm_Wstar = Wstar / torch.norm(Wstar, p=2, dim=0, keepdim=True)\n",
    "        \n",
    "        cosine_similarities = torch.mm(norm_z, norm_Wstar)\n",
    "        # Note that `CrossEntropyLoss()` = `LogSoftmax` + `NLLLoss`\n",
    "        return cosine_similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(embedding_feature_size=64, n_classes=9).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayRate = 1.0\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, \n",
    "                                                         gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature parameter\n",
    "tau = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "study effects of the temperature parameter\n",
    "- Note that the temperature parameters of 0.1 / 0.2 result in the best performance given the SimCLR and MoCo-v2 papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "yhat = model(x.to(device))\n",
    "yhat = yhat[0, :].to(\"cpu\").detach().numpy()\n",
    "x_ = np.arange(yhat.size)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9, 6))\n",
    "ax[0,0].bar(x_, softmax(yhat), label=r\"w/o $\\tau$\"); ax[0,0].legend();\n",
    "ax[1,0].bar(x_, softmax(yhat/0.1), label=r\"$\\tau$ = 0.1\"); ax[1,0].legend();\n",
    "ax[1,1].bar(x_, softmax(yhat/0.2), label=r\"$\\tau$ = 0.2\"); ax[1,1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "n_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_out_a_class(x, y, left_class):\n",
    "    \"leave out some class for the few-shot learning\"\n",
    "    indices = (y != left_class)\n",
    "    return x[indices, :, :, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.544, test_acc: 0.972 | lr: 0.0010\n",
      "epoch: 1, loss: 0.085, test_acc: 0.983 | lr: 0.0010\n",
      "epoch: 2, loss: 0.066, test_acc: 0.985 | lr: 0.0010\n",
      "epoch: 3, loss: 0.054, test_acc: 0.986 | lr: 0.0010\n",
      "epoch: 4, loss: 0.045, test_acc: 0.988 | lr: 0.0010\n",
      "epoch: 5, loss: 0.038, test_acc: 0.990 | lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "train_hist = {\"epochs\": [], \"loss_per_epoch\": [], \"loss\": [], \"test_acc\": []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    loss_per_epoch = 0\n",
    "    iters = 0\n",
    "    for x, y in train_data_loader:\n",
    "        x, y = leave_out_a_class(x, y, left_class)\n",
    "        y = torch.tensor(label_encoder.transform(y))\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = model(x)\n",
    "        \n",
    "        loss = criterion(yhat/tau, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # data storage\n",
    "        loss_per_epoch += loss\n",
    "        iters += 1\n",
    "        train_hist[\"loss\"].append(loss)\n",
    "        #print(round(loss.item(), 2), end=\" \")\n",
    "            \n",
    "    train_hist[\"epochs\"].append(epoch)\n",
    "    train_hist[\"loss_per_epoch\"].append(loss_per_epoch)\n",
    "    \n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_acc = 0\n",
    "        for x_test, y_test in test_data_loader:\n",
    "            x_test, y_test = leave_out_a_class(x_test, y_test, left_class)\n",
    "            y_test = torch.tensor(label_encoder.transform(y_test))\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "            test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "        train_hist[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    my_lr_scheduler.step()\n",
    "    \n",
    "    print(\"epoch: {}, loss: {:0.3f}, test_acc: {:0.3f} | lr: {:0.4f}\".format(epoch, loss_per_epoch/iters, test_acc, optimizer.param_groups[0]['lr']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make `support set` and `query set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = (mnist_test.targets == left_class)\n",
    "\n",
    "class SupportSet(Dataset):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        support_set_X = []\n",
    "        support_set_Y = []\n",
    "        for c in range(0, 10):\n",
    "            indices = (mnist_test.targets == c)\n",
    "            support_set_X.append( mnist_test.data[indices, :, :][:N, :, :].numpy() / 255. )\n",
    "            support_set_Y.append( mnist_test.targets[indices][:N].numpy() )\n",
    "\n",
    "        self.support_set_X = torch.from_numpy(np.array(support_set_X)).view(N*10, 1, 28, 28)\n",
    "        self.support_set_Y = torch.from_numpy(np.array(support_set_Y).flatten())\n",
    "\n",
    "        self.len = self.support_set_X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.support_set_X[idx], self.support_set_Y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "support_set = SupportSet()\n",
    "support_set_data_loader = DataLoader(support_set, batch_size=2**10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain $w_{left-class}$\n",
    "Note $w_i \\in W^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    avg_z = 0.\n",
    "    count = 0\n",
    "    for x, y in support_set:\n",
    "        if y.item() == left_class:\n",
    "            enc_yhat = model(x.view(1, 1, 28, 28).to(device, dtype=torch.float))\n",
    "            avg_z += model.z\n",
    "            count += 1\n",
    "    avg_z /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-establish $W^*$ \n",
    "- `model.Wstar_layer.weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Wstar_layer.weight.shape  # [in, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "font_Wstar = model.Wstar_layer.weight[:left_class, :]\n",
    "back_Wstar = model.Wstar_layer.weight[left_class:, :]\n",
    "\n",
    "print(font_Wstar.shape); print(back_Wstar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n"
     ]
    }
   ],
   "source": [
    "res_Wstar = torch.cat((font_Wstar, avg_z, back_Wstar), 0).to(device)\n",
    "print(res_Wstar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the `res_Wstar` to the existing model\n",
    "model.Wstar_layer.weight = nn.Parameter(res_Wstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the learned representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_class` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    indices = (mnist_test.targets == left_class)\n",
    "    \n",
    "    test_acc = 0\n",
    "    count = 0\n",
    "    for x_test, y_test in test_data_loader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        x_test, y_test = x_test[indices,:,:], y_test[indices]\n",
    "        \n",
    "        yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "        \n",
    "        test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "        count += 1\n",
    "        \n",
    "    test_acc /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8608949416342413"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "over all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0 | acc: 0.999\n",
      "class: 1 | acc: 0.999\n",
      "class: 2 | acc: 0.978\n",
      "class: 3 | acc: 0.983\n",
      "class: 4 | acc: 0.997\n",
      "class: 5 | acc: 0.992\n",
      "class: 6 | acc: 0.986\n",
      "class: 7 | acc: 0.861\n",
      "class: 8 | acc: 0.975\n",
      "class: 9 | acc: 0.972\n",
      "\n",
      " overall acc: 0.974\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    test_accs = []\n",
    "    for c in range(10):\n",
    "        indices = (mnist_test.targets == c)\n",
    "    \n",
    "        test_acc = 0\n",
    "        count = 0\n",
    "        for x_test, y_test in test_data_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            x_test, y_test = x_test[indices,:,:], y_test[indices]\n",
    "\n",
    "            yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "\n",
    "            test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "            count += 1\n",
    "\n",
    "        test_acc /= count\n",
    "        test_accs.append(test_acc)\n",
    "        print(f\"class: {c} | acc: {round(test_acc, 3)}\")\n",
    "\n",
    "print(\"\\n overall acc: {:0.3f}\".format(np.mean(test_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: With the `temperature`, the quality of the learned representations is much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
