{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine-similarity classifier [PyTorch]\n",
    "- feature extractor: ConvNet\n",
    "- classifier: cosine classifier (S. Gidaris et al., 2018)\n",
    "\n",
    "<b>Goal</b>: See if it can learn the representations effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import relu\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU settings\n",
    "device = torch.device(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Learning Settings\n",
    "K-shot N-ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "N = 1\n",
    "left_class = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# import the `MNIST datasets`\n",
    "mnist_train = dsets.MNIST(root='data',\n",
    "                          train=True,\n",
    "                          transform=data_transform,\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='data',\n",
    "                          train=False,\n",
    "                          transform=data_transform,\n",
    "                          download=True)\n",
    "\n",
    "# build the `DataLoader`\n",
    "train_data_loader = DataLoader(mnist_train, batch_size=2**10)\n",
    "test_data_loader = DataLoader(mnist_test, batch_size=mnist_test.data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daesoo\\AppData\\Local\\Programs\\Python\\Python37\\venvs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Label Encoder\n",
    "label_encoder  = preprocessing.LabelEncoder()\n",
    "\n",
    "targets = list(range(0, 10, 1))\n",
    "targets.pop(left_class)\n",
    "targets = np.array(targets).reshape(-1, 1)\n",
    "\n",
    "label_encoder.fit(targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "tensor([5, 0, 4,  ..., 3, 7, 7])\n",
      "tensor(1.)\n",
      "torch.Size([1024, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_data_loader:\n",
    "    print(x); print(y); print(x.max()); print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size=28, embedding_feature_size=2, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Data properties\n",
    "        in_channels = 1\n",
    "        \n",
    "        # Define layers\n",
    "        # 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, 12, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 2\n",
    "        self.conv2 = nn.Conv2d(12, 24, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3\n",
    "        self.conv3 = nn.Conv2d(24, 48, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 4\n",
    "        self.last_embedding_layer = nn.Linear(48*3*3, embedding_feature_size)\n",
    "        \n",
    "        # Each `w` is a representation vector of each class.\n",
    "        self.Wstar_layer = nn.Linear(embedding_feature_size, n_classes, bias=False)  # 'bias=False' allows the weights to be the pure representation vectors (not affected by the bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #============================\n",
    "        # Feature Extractor\n",
    "        h1 = self.maxpool1(relu(self.bn1(self.conv1(x))))\n",
    "        h2 = self.maxpool2(relu(self.bn2(self.conv2(h1))))\n",
    "        h3 = self.maxpool3(relu(self.bn3(self.conv3(h2))))\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        h3 = h3.view(batch_size, -1)  # 1\n",
    "        \n",
    "        self.z = self.last_embedding_layer(h3)\n",
    "        norm_z = self.z / torch.norm(self.z, p=2, dim=1, keepdim=True)#.detach() # 2)\n",
    "        \n",
    "        # 1) flattne layer\n",
    "        # 2) representation vector of `x`; Note `.detach()`: ; We don't want the l2-norm function to be involved with the gradient descent update.\n",
    "        \n",
    "        #============================\n",
    "        # Cosine-similarity Classifier\n",
    "        Wstar = self.Wstar_layer.weight.T#.detach()  # Note `.detach()`\n",
    "        norm_Wstar = Wstar / torch.norm(Wstar, p=2, dim=0, keepdim=True)\n",
    "        \n",
    "        cosine_similarities = torch.mm(norm_z, norm_Wstar)\n",
    "        # Note that `CrossEntropyLoss()` = `LogSoftmax` + `NLLLoss`\n",
    "        return cosine_similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(embedding_feature_size=64, n_classes=9).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "n_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_out_a_class(x, y, left_class):\n",
    "    \"leave out some class for the few-shot learning\"\n",
    "    indices = (y != left_class)\n",
    "    return x[indices, :, :, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.572, test_acc: 0.964\n",
      "epoch: 1, loss: 1.382, test_acc: 0.978\n",
      "epoch: 2, loss: 1.357, test_acc: 0.983\n",
      "epoch: 3, loss: 1.345, test_acc: 0.986\n",
      "epoch: 4, loss: 1.336, test_acc: 0.988\n",
      "epoch: 5, loss: 1.330, test_acc: 0.987\n"
     ]
    }
   ],
   "source": [
    "train_hist = {\"epochs\": [], \"loss_per_epoch\": [], \"loss\": [], \"test_acc\": []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    loss_per_epoch = 0\n",
    "    iters = 0\n",
    "    for x, y in train_data_loader:\n",
    "        x, y = leave_out_a_class(x, y, left_class)\n",
    "        y = torch.tensor(label_encoder.transform(y))\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = model(x)\n",
    "        \n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # data storage\n",
    "        loss_per_epoch += loss\n",
    "        iters += 1\n",
    "        train_hist[\"loss\"].append(loss)\n",
    "        #print(round(loss.item(), 2), end=\" \")\n",
    "            \n",
    "    train_hist[\"epochs\"].append(epoch)\n",
    "    train_hist[\"loss_per_epoch\"].append(loss_per_epoch)\n",
    "    \n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_acc = 0\n",
    "        for x_test, y_test in test_data_loader:\n",
    "            x_test, y_test = leave_out_a_class(x_test, y_test, left_class)\n",
    "            y_test = torch.tensor(label_encoder.transform(y_test))\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "            test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "        train_hist[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    print(\"epoch: {}, loss: {:0.3f}, test_acc: {:0.3f}\".format(epoch, loss_per_epoch/iters, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make `support set` and `query set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = (mnist_test.targets == left_class)\n",
    "\n",
    "class SupportSet(Dataset):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        support_set_X = []\n",
    "        support_set_Y = []\n",
    "        for c in range(0, 10):\n",
    "            indices = (mnist_test.targets == c)\n",
    "            support_set_X.append( mnist_test.data[indices, :, :][:N, :, :].numpy() / 255. )\n",
    "            support_set_Y.append( mnist_test.targets[indices][:N].numpy() )\n",
    "\n",
    "        self.support_set_X = torch.from_numpy(np.array(support_set_X)).view(N*10, 1, 28, 28)\n",
    "        self.support_set_Y = torch.from_numpy(np.array(support_set_Y).flatten())\n",
    "\n",
    "        self.len = self.support_set_X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.support_set_X[idx], self.support_set_Y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "support_set = SupportSet()\n",
    "support_set_data_loader = DataLoader(support_set, batch_size=2**10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain $w_{left-class}$\n",
    "Note $w_i \\in W^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    avg_z = 0.\n",
    "    count = 0\n",
    "    for x, y in support_set:\n",
    "        if y.item() == left_class:\n",
    "            enc_yhat = model(x.view(1, 1, 28, 28).to(device, dtype=torch.float))\n",
    "            avg_z += model.z\n",
    "            count += 1\n",
    "    avg_z /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-establish $W^*$ \n",
    "- `model.Wstar_layer.weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Wstar_layer.weight.shape  # [in, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "font_Wstar = model.Wstar_layer.weight[:left_class, :]\n",
    "back_Wstar = model.Wstar_layer.weight[left_class:, :]\n",
    "\n",
    "print(font_Wstar.shape); print(back_Wstar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n"
     ]
    }
   ],
   "source": [
    "res_Wstar = torch.cat((font_Wstar, avg_z, back_Wstar), 0).to(device)\n",
    "print(res_Wstar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the `res_Wstar` to the existing model\n",
    "model.Wstar_layer.weight = nn.Parameter(res_Wstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the learned representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_class` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    indices = (mnist_test.targets == left_class)\n",
    "    \n",
    "    test_acc = 0\n",
    "    count = 0\n",
    "    for x_test, y_test in test_data_loader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        x_test, y_test = x_test[indices,:,:], y_test[indices]\n",
    "        \n",
    "        yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "        \n",
    "        test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "        count += 1\n",
    "        \n",
    "    test_acc /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5223735408560312"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "over all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0 | acc: 0.991\n",
      "class: 1 | acc: 0.992\n",
      "class: 2 | acc: 0.99\n",
      "class: 3 | acc: 0.99\n",
      "class: 4 | acc: 0.976\n",
      "class: 5 | acc: 0.988\n",
      "class: 6 | acc: 0.983\n",
      "class: 7 | acc: 0.522\n",
      "class: 8 | acc: 0.99\n",
      "class: 9 | acc: 0.979\n",
      "\n",
      " overall acc: 0.940\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    test_accs = []\n",
    "    for c in range(10):\n",
    "        indices = (mnist_test.targets == c)\n",
    "    \n",
    "        test_acc = 0\n",
    "        count = 0\n",
    "        for x_test, y_test in test_data_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            x_test, y_test = x_test[indices,:,:], y_test[indices]\n",
    "\n",
    "            yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "\n",
    "            test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "            count += 1\n",
    "\n",
    "        test_acc /= count\n",
    "        test_accs.append(test_acc)\n",
    "        print(f\"class: {c} | acc: {round(test_acc, 3)}\")\n",
    "\n",
    "print(\"\\n overall acc: {:0.3f}\".format(np.mean(test_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: Given that there are 10 classes, the random-chance of getting `left_class` right is 10%. This model is not even trained but just uses the <i>prototype</i>. Hence, The result if is not so bad compared to the <i>linear classifier</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight | grad:False\n",
      "conv1.bias | grad:False\n",
      "bn1.weight | grad:False\n",
      "bn1.bias | grad:False\n",
      "conv2.weight | grad:False\n",
      "conv2.bias | grad:False\n",
      "bn2.weight | grad:False\n",
      "bn2.bias | grad:False\n",
      "conv3.weight | grad:False\n",
      "conv3.bias | grad:False\n",
      "bn3.weight | grad:False\n",
      "bn3.bias | grad:False\n",
      "last_embedding_layer.weight | grad:False\n",
      "last_embedding_layer.bias | grad:False\n",
      "Wstar_layer.weight | grad:True\n"
     ]
    }
   ],
   "source": [
    "# free all the layers except `Wstar_layer`\n",
    "for name, param in model.named_parameters():\n",
    "    if not name == 'Wstar_layer.weight':\n",
    "        param.requires_grad = False\n",
    "    print(name, f\"| grad:{param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad], \n",
    "                              lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = {\"epochs\": [], \"loss_per_epoch\": [], \"loss\": [], \"test_acc\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.451, test_acc: 0.940\n",
      "epoch: 10, loss: 1.440, test_acc: 0.940\n",
      "epoch: 20, loss: 1.432, test_acc: 0.941\n",
      "epoch: 30, loss: 1.426, test_acc: 0.941\n",
      "epoch: 40, loss: 1.421, test_acc: 0.942\n",
      "epoch: 50, loss: 1.417, test_acc: 0.942\n",
      "epoch: 60, loss: 1.414, test_acc: 0.942\n",
      "epoch: 70, loss: 1.412, test_acc: 0.942\n",
      "epoch: 80, loss: 1.410, test_acc: 0.942\n",
      "epoch: 90, loss: 1.409, test_acc: 0.942\n",
      "epoch: 100, loss: 1.408, test_acc: 0.942\n",
      "epoch: 110, loss: 1.407, test_acc: 0.942\n",
      "epoch: 120, loss: 1.406, test_acc: 0.942\n",
      "epoch: 130, loss: 1.405, test_acc: 0.941\n",
      "epoch: 140, loss: 1.405, test_acc: 0.941\n",
      "epoch: 150, loss: 1.404, test_acc: 0.940\n",
      "epoch: 160, loss: 1.404, test_acc: 0.939\n",
      "epoch: 170, loss: 1.404, test_acc: 0.939\n",
      "epoch: 180, loss: 1.403, test_acc: 0.939\n",
      "epoch: 190, loss: 1.403, test_acc: 0.938\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    loss_per_epoch = 0\n",
    "    iters = 0\n",
    "    for x, y in support_set_data_loader:\n",
    "        x, y = x.to(device, dtype=torch.float), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = model(x)\n",
    "        \n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # data storage\n",
    "        loss_per_epoch += loss\n",
    "        iters += 1\n",
    "        train_hist[\"loss\"].append(loss)\n",
    "        #print(round(loss.item(), 2), end=\" \")\n",
    "            \n",
    "    train_hist[\"epochs\"].append(epoch)\n",
    "    train_hist[\"loss_per_epoch\"].append(loss_per_epoch)\n",
    "    \n",
    "    # validation\n",
    "    if (epoch % 10) == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            test_acc = 0\n",
    "            for x_test, y_test in test_data_loader:\n",
    "                x_test, y_test = x_test.to(device, dtype=torch.float), y_test.to(device)\n",
    "                yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "                test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "            train_hist[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        print(\"epoch: {}, loss: {:0.3f}, test_acc: {:0.3f}\".format(epoch, loss_per_epoch/iters, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the learned representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`left_class` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    indices = (mnist_test.targets == left_class)\n",
    "    \n",
    "    test_acc = 0\n",
    "    count = 0\n",
    "    for x_test, y_test in test_data_loader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        x_test, y_test = x_test[indices,:,:], y_test[indices]\n",
    "        \n",
    "        yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "        \n",
    "        test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "        count += 1\n",
    "        \n",
    "    test_acc /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5953307392996109"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "over all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0 | acc: 0.994\n",
      "class: 1 | acc: 0.879\n",
      "class: 2 | acc: 0.978\n",
      "class: 3 | acc: 0.767\n",
      "class: 4 | acc: 0.95\n",
      "class: 5 | acc: 0.929\n",
      "class: 6 | acc: 0.952\n",
      "class: 7 | acc: 0.595\n",
      "class: 8 | acc: 0.939\n",
      "class: 9 | acc: 0.964\n",
      "\n",
      " overall acc: 0.895\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    test_accs = []\n",
    "    for c in range(10):\n",
    "        indices = (mnist_test.targets == c)\n",
    "    \n",
    "        test_acc = 0\n",
    "        count = 0\n",
    "        for x_test, y_test in test_data_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            x_test, y_test = x_test[indices,:,:], y_test[indices]\n",
    "\n",
    "            yhat = torch.argmax(model(x_test.to(device)), axis=1)\n",
    "\n",
    "            test_acc += np.mean((yhat.to(\"cpu\") == y_test.to(\"cpu\")).numpy())\n",
    "            count += 1\n",
    "\n",
    "        test_acc /= count\n",
    "        test_accs.append(test_acc)\n",
    "        print(f\"class: {c} | acc: {round(test_acc, 3)}\")\n",
    "\n",
    "print(\"\\n overall acc: {:0.3f}\".format(np.mean(test_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: The performance degraded than the prior approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
